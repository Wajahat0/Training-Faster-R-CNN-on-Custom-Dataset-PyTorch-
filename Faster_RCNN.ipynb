{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ec4dfa-ac61-4330-9300-eedf6ac9b62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a COCO-pretrained YOLO11n model\n",
    "# model = YOLO(r\"C:\\Users\\wajah\\Downloads\\ALL\\runs\\detect\\train5\\weights\\last.pt\")\n",
    "\n",
    "from ultralytics import RTDETR\n",
    "\n",
    "# Load a COCO-pretrained RT-DETR-l model\n",
    "model = YOLO(\"yolo11m.pt\")\n",
    "\n",
    "# Train the model on the COCO8 example dataset for 100 epochs\n",
    "results = model.train(data=r\"D:\\ALL\\customdatabase.yaml\", epochs=100, imgsz=640,batch=-1)\n",
    "\n",
    "# results = model.train(resume=True)\n",
    "\n",
    "# Run inference with the YOLO11n model on the 'bus.jpg' image\n",
    "# results = model(\"path/to/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn,fasterrcnn_resnet50_fpn_v2\n",
    "from torchvision.models.detection.rpn import AnchorGenerator\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection import faster_rcnn\n",
    "# from torchvision.models.detection import BackboneWithFPN\n",
    "import numpy as np\n",
    "from torchvision.datasets import CocoDetection\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, datasets, tv_tensors\n",
    "from helpers import plot\n",
    "from torchvision.transforms import v2\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "from torch.utils.data import ConcatDataset, DataLoader\n",
    "\n",
    "# Ensure you have the correct COCO dataset, or use a different dataset\n",
    "# This example assumes you're using COCO format dataset, but similar for VOC\n",
    "coco_gt = COCO(r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\json_labels\\test.json\")\n",
    "\n",
    "# Data Transformations\n",
    "transforms = v2.Compose(\n",
    "    [\n",
    "        # v2.RandomApply(transforms=[v2.GaussianBlur(kernel_size=(5, 5), sigma=(0.5, 3.0))],p=0.5),\n",
    "        v2.ToImage(),\n",
    "        # v2.RandomPhotometricDistort(p=1),\n",
    "        # v2.RandomRotation(degrees=(-30, 30)),\n",
    "        # v2.RandomErasing(),\n",
    "        # v2.JPEG((10)),\n",
    "        # v2.RandomApply(transforms=[v2.JPEG((10))],p=1.0),\n",
    "        # v2.RandomApply(transforms=[v2.JPEG((20,60))],p=0.3),\n",
    "        v2.RandomApply(transforms=[v2.RandomIoUCrop()],p=0.5),\n",
    "        v2.RandomPerspective(distortion_scale=0.3,fill={tv_tensors.Image: (123, 117, 104), \"others\": 0}, p=0.5),\n",
    "        v2.RandomZoomOut(fill={tv_tensors.Image: (123, 117, 104), \"others\": 0},side_range=(1.0, 2.0),p=0.5),\n",
    "        v2.RandomApply(transforms=[v2.ColorJitter(brightness=0.5,contrast=0.3,saturation=0.3)],p=0.5),\n",
    "        v2.RandomHorizontalFlip(p=1),\n",
    "        v2.Resize((640,640),antialias=True),\n",
    "        v2.SanitizeBoundingBoxes(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        # v2.RandomApply(transforms=[v2.GaussianNoise(sigma=0.05)],p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_transforms = v2.Compose(\n",
    "    [\n",
    "        v2.ToImage(),\n",
    "        # v2.SanitizeBoundingBoxes(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Load COCO Dataset (you can change it for VOC dataset)\n",
    "train_dataset = CocoDetection(\n",
    "    root=r\"D:\\ALL\\images\\train\",\n",
    "    annFile=r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\json_labels\\train.json\",\n",
    "    transforms=transforms\n",
    ")\n",
    "\n",
    "# train_dataset_av = CocoDetection(\n",
    "#     root=r'C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\adv_images\\train',\n",
    "#     annFile=r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\json_labels\\train.json\",\n",
    "#     transforms=transforms\n",
    "# )\n",
    "\n",
    "\n",
    "dataset_0 = datasets.wrap_dataset_for_transforms_v2(train_dataset, target_keys=(\"boxes\", \"labels\"))\n",
    "# dataset_1 = datasets.wrap_dataset_for_transforms_v2(train_dataset_av, target_keys=(\"boxes\", \"labels\"))\n",
    "# print(len(dataset_0))\n",
    "# combined_dataset = ConcatDataset([dataset_0, dataset_1])\n",
    "# print(len(combined_dataset))\n",
    "# Load COCO Dataset (you can change it for VOC dataset)\n",
    "test_dataset = CocoDetection(\n",
    "    root=r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\images\\test\",\n",
    "    annFile=r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\json_labels\\test.json\",\n",
    "    transforms=test_transforms\n",
    ")\n",
    "\n",
    "test_dataset = datasets.wrap_dataset_for_transforms_v2(test_dataset, target_keys=(\"boxes\", \"labels\",\"image_id\"))\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_0,\n",
    "    batch_size=4,\n",
    "    shuffle=True,\n",
    "    # We need a custom collation function here, since the object detection\n",
    "    # models expect a sequence of images and target dictionaries. The default\n",
    "    # collation function tries to torch.stack() the individual elements,\n",
    "    # which fails in general for object detection, because the number of bounding\n",
    "    # boxes varies between the images of the same batch.\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    # We need a custom collation function here, since the object detection\n",
    "    # models expect a sequence of images and target dictionaries. The default\n",
    "    # collation function tries to torch.stack() the individual elements,\n",
    "    # which fails in general for object detection, because the number of bounding\n",
    "    # boxes varies between the images of the same batch.\n",
    "    collate_fn=lambda batch: tuple(zip(*batch)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd1ba57",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = test_dataset[0]\n",
    "img, target = sample\n",
    "print(f\"{type(img) = }\\n{type(target) = }\\n{target.keys() = }\")\n",
    "print(f\"{type(target['boxes']) = }\\n{type(target['labels']) = }\")\n",
    "plot([dataset_0[1], dataset_0[2]])\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7af08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='COCO_V1')\n",
    "    # model= torchvision.models.detection.retinanet_resnet50_fpn_v2(weights='COCO_V1')\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    return model\n",
    "\n",
    "num_classes = 15\n",
    "model = create_model(num_classes)\n",
    "\n",
    "# model.load_state_dict(torch.load(r\"C:\\Users\\wajah\\Downloads\\ALL\\best_model(FasterRCNN)_adamw_640_50.pth\"))\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# model= torchvision.models.detection.retinanet_resnet50_fpn_v2(weights=None,num_classes=15, weights_backbone = ResNet50_Weights.IMAGENET1K_V2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88c57ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd48364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchvision.models.detection import FasterRCNN\n",
    "# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# # Setup\n",
    "# backbone = resnet_fpn_backbone(backbone_name='resnet50', weights=\"IMAGENET1K_V1\")\n",
    "# model = FasterRCNN(backbone=backbone, num_classes=15,min_size=640,max_size=640)\n",
    "\n",
    "\n",
    "# import torchvision\n",
    "# from torchvision.models.detection import RetinaNet\n",
    "# from torchvision.models.detection.backbone_utils import resnet_fpn_backbone\n",
    "# from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "# # Load a pretrained ResNet101 backbone with FPN\n",
    "# backbone = resnet_fpn_backbone(\n",
    "#     backbone_name='resnet50',\n",
    "#     weights=ResNet50_Weights.IMAGENET1K_V1,\n",
    "#     # trainable_layers=3  # Optional: freeze earlier layers\n",
    "# )\n",
    "\n",
    "# # Create the RetinaNet model\n",
    "# model = RetinaNet(backbone=backbone, num_classes=15,min_size=640,max_size=640)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4146dee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, train_loader, optimizer, scaler, accumulation_steps, device):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training\", unit=\"batch\")\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    for i, (images, targets) in progress_bar:\n",
    "        images = [img.to(device) for img in images]\n",
    "        \n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        # Normalize loss for accumulation\n",
    "        loss = losses / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Only update weights every `accumulation_steps` iterations\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        train_loss += losses.item()  # Add actual (un-normalized) loss\n",
    "        progress_bar.set_postfix(loss=losses.item())\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader)\n",
    "    return avg_train_loss\n",
    "\n",
    "\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import numpy as np\n",
    "\n",
    "def validate(model, val_loader, device, coco_gt):\n",
    "    model.eval()  # Use eval mode for evaluation\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    annotation_id = 0  # Unique ID across all predictions\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in val_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for i, output in enumerate(outputs):\n",
    "                image_id = targets[i]['image_id']\n",
    "                boxes = output['boxes'].cpu().numpy()\n",
    "                scores = output['scores'].cpu().numpy()\n",
    "                labels = output['labels'].cpu().numpy()\n",
    "\n",
    "                for box, score, label in zip(boxes, scores, labels):\n",
    "                    x_min, y_min, x_max, y_max = box\n",
    "                    width = x_max - x_min\n",
    "                    height = y_max - y_min\n",
    "\n",
    "                    predictions.append({\n",
    "                        'image_id': image_id,\n",
    "                        'category_id': int(label),\n",
    "                        'bbox': [x_min, y_min, width, height],\n",
    "                        'score': float(score)\n",
    "                    })\n",
    "\n",
    "    # Evaluate with COCO API\n",
    "    coco_dt = coco_gt.loadRes(predictions)\n",
    "    coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    mAP_50_95 = coco_eval.stats[0]  # mAP@[0.50:0.95]\n",
    "    mAP_50 = coco_eval.stats[1]     # mAP@0.50\n",
    "\n",
    "    return mAP_50, mAP_50_95\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, optimizer, scaler, accumulation_steps,\n",
    "                       lr_scheduler, num_epochs, device, coco_gt, model_save_path='best_model.pth'):\n",
    "    best_map = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    patience = 50  # Early stopping patience\n",
    "    # avg_train_loss=0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "        # Training\n",
    "        avg_train_loss = train_one_epoch(model, train_loader, optimizer, scaler, accumulation_steps, device)\n",
    "\n",
    "        # Validation - returns mAP@50 and mAP@[.50:.95]\n",
    "        mAP_50, mAP_50_95 = validate(model, val_loader, device, coco_gt)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {avg_train_loss:.4f} - \"\n",
    "              f\"mAP@50: {mAP_50:.4f} - mAP@[.50:.95]: {mAP_50_95:.4f}\")\n",
    "\n",
    "        # Save the model if mAP@[.50:.95] improved\n",
    "        if mAP_50 > best_map:\n",
    "            print(\"Validation mAP improved, saving the model...\")\n",
    "            best_map = mAP_50\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "        # Early stopping\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping after {epochs_no_improve} epochs without improvement.\")\n",
    "            break\n",
    "\n",
    "        # Update learning rate scheduler\n",
    "        lr_scheduler.step()\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92c8785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def find_lr(model, train_loader, optimizer, device, init_value=1e-7, final_value=10., beta=0.98):\n",
    "    num = len(train_loader) - 1\n",
    "    mult = (final_value / init_value) ** (1/num)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0]['lr'] = lr\n",
    "    avg_loss = 0.\n",
    "    best_loss = float('inf')\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch_num, (images, targets) in enumerate(train_loader):\n",
    "        images = list(img.to(device) for img in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        avg_loss = beta * avg_loss + (1 - beta) * loss.item()\n",
    "        smoothed_loss = avg_loss / (1 - beta**(batch_num + 1))\n",
    "\n",
    "        log_lrs.append(np.log10(lr))\n",
    "        losses.append(smoothed_loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        lr *= mult\n",
    "        optimizer.param_groups[0]['lr'] = lr\n",
    "\n",
    "        if batch_num > 1 and smoothed_loss > 4 * best_loss:\n",
    "            break\n",
    "\n",
    "        if smoothed_loss < best_loss or batch_num == 0:\n",
    "            best_loss = smoothed_loss\n",
    "\n",
    "    return log_lrs, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86e1f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04afc6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "optimizer = torch.optim.AdamW(params, lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "# Mixed precision scaler\n",
    "scaler = GradScaler()\n",
    "\n",
    "# Gradient accumulation setup\n",
    "accumulation_steps = 4\n",
    "num_epochs = 100\n",
    "# Define the save path for the best model\n",
    "model_save_path = 'Faster_v2_model_adamw_640_50.pth'\n",
    "\n",
    "# log_lrs, losses= find_lr(model, train_loader, optimizer, device, init_value=1e-5, final_value=10.0, beta=0.98)\n",
    "\n",
    "# # # log_lrs, losses = find_lr(model, train_loader, optimizer, device)\n",
    "# # # \n",
    "# plt.plot(log_lrs, losses)\n",
    "# plt.xlabel(\"Log10 Learning Rate\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Learning Rate Finder\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n",
    "\n",
    "# Train and evaluate\n",
    "train_and_evaluate(model, train_loader, test_loader, optimizer, scaler, accumulation_steps, lr_scheduler, num_epochs, device,coco_gt, model_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fd313b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C\n",
    "train_and_evaluate(model, train_loader, test_loader, optimizer, scaler, accumulation_steps, lr_scheduler, num_epochs, device,coco_gt, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55b40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e55992-2510-4371-82e5-9d2abd36c0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.transforms import v2 as T\n",
    "from torchvision import datasets\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "import json\n",
    "import torchvision\n",
    "\n",
    "# ----------------------------\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------------------\n",
    "# Define test transforms\n",
    "test_transforms = T.Compose([\n",
    "    T.ToImage(),\n",
    "    T.ToDtype(torch.float32, scale=True),\n",
    "    # T.Resize((512, 512)),\n",
    "    # T.ToTensor()\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# Load COCO annotation and dataset\n",
    "coco = COCO(r\"C:\\Users\\wajah\\Downloads\\ALL\\H_40X_C2\\json_labels\\test.json\")\n",
    "\n",
    "test_dataset = CocoDetection(\n",
    "    root=r'C:\\Users\\wajah\\Downloads\\ALL\\H_40X_C2\\images\\test',\n",
    "    annFile=r\"C:\\Users\\wajah\\Downloads\\ALL\\H_40X_C2\\json_labels\\test.json\",\n",
    "    transforms=test_transforms\n",
    ")\n",
    "\n",
    "# Optional: wrap dataset for TorchVision v2\n",
    "dataset = datasets.wrap_dataset_for_transforms_v2(test_dataset, target_keys=(\"boxes\", \"labels\", \"image_id\"))\n",
    "\n",
    "# ----------------------------\n",
    "# Load Faster R-CNN model\n",
    "model =torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=15)  # <-- change num_classes based on your dataset\n",
    "\n",
    "# fasterrcnn_resnet50_fpn\n",
    "# model.load_state_dict(torch.load(\"final_model.pth\", map_location=device))\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\wajah\\Downloads\\ALL\\Faster_v2_model_adamw_640_50.pth\", map_location=device))\n",
    "# Define the save path for the best model\n",
    "# model_save_path = 'best_model_adamw_640_50.pth'\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# ----------------------------\n",
    "# Run inference and build predictions in COCO format\n",
    "predictions = []\n",
    "\n",
    "for idx in range(len(dataset)):\n",
    "    image, target = dataset[idx]\n",
    "\n",
    "    image = image.to(device)\n",
    "    image_id = target[\"image_id\"].item() if isinstance(target[\"image_id\"], torch.Tensor) else target[\"image_id\"]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model([image])\n",
    "\n",
    "    output = outputs[0]\n",
    "\n",
    "    for box, score, label in zip(output[\"boxes\"], output[\"scores\"], output[\"labels\"]):\n",
    "        x1, y1, x2, y2 = box.tolist()\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "        coco_box = [x1, y1, width, height]\n",
    "\n",
    "        predictions.append({\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": int(label.item()),\n",
    "            \"bbox\": coco_box,\n",
    "            \"score\": float(score.item())\n",
    "        })\n",
    "\n",
    "# ----------------------------\n",
    "# Load predictions into COCOeval\n",
    "coco_dt = coco.loadRes(predictions)\n",
    "coco_eval = COCOeval(coco, coco_dt, iouType='bbox')\n",
    "\n",
    "# Evaluate and summarize\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bcc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3b911a-0eb4-4b69-a8c9-ec04fe4b8412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# --- CONFIG ---\n",
    "input_dir = r'C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\images\\train'         # Folder with images\n",
    "label_dir = r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\txt_labels\\WBC_Detection\\train\"         # Folder with YOLO .txt files\n",
    "output_dir = r'C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2'   # Folder to save COCO JSON\n",
    "\n",
    "# --- CREATE 14 CATEGORY ENTRIES ---\n",
    "categories = [{\"id\": i + 1, \"name\": f\"class_{i}\", \"supercategory\": \"none\"} for i in range(14)]\n",
    "\n",
    "# --- INIT COCO STRUCTURE ---\n",
    "coco_dataset = {\n",
    "    \"info\": {\"description\": \"YOLO to COCO Converted Dataset\"},\n",
    "    \"licenses\": [],\n",
    "    \"categories\": categories,\n",
    "    \"images\": [],\n",
    "    \"annotations\": []\n",
    "}\n",
    "\n",
    "annotation_id = 0\n",
    "image_id = 0\n",
    "\n",
    "# --- LOOP OVER IMAGES ---\n",
    "for image_file in os.listdir(input_dir):\n",
    "    if not image_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "        continue  # skip non-images\n",
    "\n",
    "    image_path = os.path.join(input_dir, image_file)\n",
    "    label_path = os.path.join(label_dir, os.path.splitext(image_file)[0] + \".txt\")\n",
    "\n",
    "    if not os.path.exists(label_path):\n",
    "        continue  # skip if label not found\n",
    "\n",
    "    # Read image to get dimensions\n",
    "    with Image.open(image_path) as img:\n",
    "        width, height = img.size\n",
    "\n",
    "    image_id += 1\n",
    "    coco_dataset[\"images\"].append({\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_file,\n",
    "        \"width\": width,\n",
    "        \"height\": height\n",
    "    })\n",
    "\n",
    "    # Read YOLO labels\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) != 5:\n",
    "            continue\n",
    "\n",
    "        class_id, x_center, y_center, w, h = map(float, parts)\n",
    "        x_center *= width\n",
    "        y_center *= height\n",
    "        w *= width\n",
    "        h *= height\n",
    "        x_min = x_center - w / 2\n",
    "        y_min = y_center - h / 2\n",
    "\n",
    "        annotation_id += 1\n",
    "        coco_dataset[\"annotations\"].append({\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": int(class_id) + 1,  # YOLO class 0 → COCO category 1\n",
    "            \"bbox\": [x_min, y_min, w, h],\n",
    "            \"area\": w * h,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "\n",
    "# --- SAVE JSON ---\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, 'annotations.json'), 'w') as f:\n",
    "    json.dump(coco_dataset, f, indent=4)\n",
    "\n",
    "print(f\"✅ COCO annotation file saved to: {os.path.join(output_dir, 'annotations.json')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80ad755-fce9-4ac6-98ab-405a344d01b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from ultralytics import YOLO\n",
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch\n",
    "# import torchvision.models as models\n",
    "\n",
    "# model = YOLO(\"yolo11l-cls.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# # Train the model\n",
    "# results = model.train(data=r\"D:\\ALL\", epochs=100, imgsz=224,degrees=30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8066a23-258d-4ec6-b5ee-5e5d7167a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import cv2\n",
    "\n",
    "# # Paths\n",
    "# image_dir = r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\images\\train\"\n",
    "# label_dir = r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\labels_mul\\train\"\n",
    "# output_dir = r\"D:\\ALL\\train1/\"\n",
    "\n",
    "# # Create output directory if it doesn't exist\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# # Loop through all annotation files\n",
    "# for label_file in os.listdir(label_dir):\n",
    "#     if not label_file.endswith(\".txt\"):\n",
    "#         continue\n",
    "\n",
    "#     image_name = os.path.splitext(label_file)[0] + \".png\"\n",
    "#     image_path = os.path.join(image_dir, image_name)\n",
    "#     label_path = os.path.join(label_dir, label_file)\n",
    "\n",
    "#     if not os.path.exists(image_path):\n",
    "#         print(f\"Image {image_path} not found.\")\n",
    "#         continue\n",
    "\n",
    "#     # Load image\n",
    "#     img = cv2.imread(image_path)\n",
    "#     h_img, w_img, _ = img.shape\n",
    "\n",
    "#     # Read label file\n",
    "#     with open(label_path, \"r\") as f:\n",
    "#         lines = f.readlines()\n",
    "\n",
    "#     for idx, line in enumerate(lines):\n",
    "#         parts = line.strip().split()\n",
    "#         if len(parts) < 5:\n",
    "#             continue  # skip malformed lines\n",
    "\n",
    "#         class_id = parts[0]\n",
    "#         cx, cy, bw, bh = map(float, parts[1:5])\n",
    "\n",
    "#         # Convert normalized to absolute coordinates\n",
    "#         x_center = int(cx * w_img)\n",
    "#         y_center = int(cy * h_img)\n",
    "#         box_w = int(bw * w_img)\n",
    "#         box_h = int(bh * h_img)\n",
    "\n",
    "#         # Determine the size to make it square\n",
    "#         side_length = max(box_w, box_h)\n",
    "\n",
    "#         # Calculate new square box coordinates\n",
    "#         x1 = x_center - side_length // 2\n",
    "#         y1 = y_center - side_length // 2\n",
    "#         x2 = x1 + side_length\n",
    "#         y2 = y1 + side_lengthimport torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Download and load the pre-trained ResNet-18 model\n",
    "# net = models.resnet50(pretrained=True)\n",
    "\n",
    "#         # Clamp to image boundaries\n",
    "#         x1 = max(0, x1)\n",
    "#         y1 = max(0, y1)\n",
    "#         x2 = min(w_img, x2)\n",
    "#         y2 = min(h_img, y2)\n",
    "\n",
    "#         # Adjust again if clamping changes size\n",
    "#         crop = img[y1:y2, x1:x2]\n",
    "#         if crop.shape[0] != crop.shape[1]:\n",
    "#             side = min(crop.shape[:2])\n",
    "#             crop = crop[:side, :side]\n",
    "\n",
    "#         if crop.size == 0:\n",
    "#             print(f\"Skipping empty crop: {image_name}, box {idx}\")\n",
    "#             continue\n",
    "\n",
    "#         # Create class-specific output folder\n",
    "#         class_folder = os.path.join(output_dir, f\"class_{class_id}\")\n",
    "#         os.makedirs(class_folder, exist_ok=True)\n",
    "\n",
    "#         # Save cropped and square image\n",
    "#         out_path = os.path.join(class_folder, f\"{os.path.splitext(image_name)[0]}_{idx}.jpg\")\n",
    "#         cv2.imwrite(out_path, crop)\n",
    "#         print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe6fd7-8c95-4b0a-b889-e41bf14f5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# input_root = r\"C:\\Users\\wajah\\Downloads\\ALL\\H_100X_C2\\labels\"\n",
    "# output_root = \"labels_roi\"\n",
    "\n",
    "# for split in [\"train\", \"test\"]:\n",
    "#     input_folder = os.path.join(input_root, split)\n",
    "#     output_folder = os.path.join(output_root, split)\n",
    "#     os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "#     for filename in os.listdir(input_folder):\n",
    "#         if not filename.endswith(\".txt\"):\n",
    "#             continue\n",
    "\n",
    "#         input_path = os.path.join(input_folder, filename)\n",
    "#         output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "#         with open(input_path, \"r\") as infile, open(output_path, \"w\") as outfile:\n",
    "#             for line in infile:\n",
    "#                 parts = line.strip().split()\n",
    "#                 if len(parts) >= 5:\n",
    "#                     # Convert class_id to 0, keep bbox and extra info\n",
    "#                     new_line = \"0 \" + \" \".join(parts[1:]) + \"\\n\"\n",
    "#                     outfile.write(new_line)\n",
    "\n",
    "# print(\"✅ All annotations in 'train' and 'test' converted to single-class format for ROI detection.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea0b68-9984-446a-86b8-73adf0cf62d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import csv\n",
    "\n",
    "# root_dir = r\"D:\\ALL\\test\"  # Main folder containing class-named subfolders\n",
    "# output_csv = r\"D:\\ALL\\test_labels.csv\"\n",
    "\n",
    "# with open(output_csv, \"w\", newline=\"\") as csvfile:\n",
    "#     writer = csv.writer(csvfile)\n",
    "#     writer.writerow([\"filename\", \"label\"])  # Header\n",
    "\n",
    "#     for label in sorted(os.listdir(root_dir)):\n",
    "#         class_path = os.path.join(root_dir, label)\n",
    "#         if not os.path.isdir(class_path):\n",
    "#             continue\n",
    "\n",
    "#         for img_file in os.listdir(class_path):\n",
    "#             if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "#                 img_path = os.path.join(label, img_file)\n",
    "#                 writer.writerow([img_path, label.split(\"_\")[1]])\n",
    "\n",
    "# print(f\"✅ CSV file saved: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85567ae8-8199-45f4-b153-551144846298",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cd165f-aa30-41c7-8d1b-3049f2a73417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch.optim as optim\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "\n",
    "# import torch\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Download and load the pre-trained ResNet-18 model\n",
    "# net = models.resnet50(pretrained=True)\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebca986-e545-4618-8900-c633d52d113e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def convert_to_yolo(json_path, output_dir):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    image_info = {img['id']: img for img in data['images']}\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create a mapping from original category_id to 0-based class index\n",
    "    category_id_to_index = {cat['id']: idx for idx, cat in enumerate(sorted(data['categories'], key=lambda x: x['id']))}\n",
    "\n",
    "    for ann in data['annotations']:\n",
    "        image_id = ann['image_id']\n",
    "        if isinstance(image_id, int):\n",
    "            img = image_info[image_id]\n",
    "            filename = img['file_name']\n",
    "        else:\n",
    "            filename = image_id\n",
    "            img = next((i for i in image_info.values() if i['file_name'] == filename), None)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "        img_w, img_h = img['width'], img['height']\n",
    "        x, y, w, h = ann['bbox']\n",
    "\n",
    "        # Convert to YOLO format\n",
    "        x_center = (x + w / 2) / img_w\n",
    "        y_center = (y + h / 2) / img_h\n",
    "        w_norm = w / img_w\n",
    "        h_norm = h / img_h\n",
    "\n",
    "        # Map original category_id to index in 0–13\n",
    "        class_id = category_id_to_index[ann['category_id']]\n",
    "\n",
    "        label_filename = os.path.splitext(filename)[0] + '.txt'\n",
    "        label_path = os.path.join(output_dir, label_filename)\n",
    "\n",
    "        with open(label_path, 'a') as f:\n",
    "            f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
    "\n",
    "    print(f\"✅ Converted {len(data['annotations'])} annotations to YOLO format in '{output_dir}'\")\n",
    "    print(f\"✅ Class IDs used: {list(category_id_to_index.values())}\")\n",
    "\n",
    "# Example usage:\n",
    "convert_to_yolo(r\"D:\\ALL\\Blance_augmented.json\", r\"D:\\ALL\\labels\\train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f2c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_classes(json_path, output_file=\"classes.txt\"):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    if \"categories\" not in data:\n",
    "        print(\"No 'categories' field found in JSON.\")\n",
    "        return\n",
    "\n",
    "    categories = sorted(data['categories'], key=lambda x: x['id'])\n",
    "    with open(output_file, 'w') as f:\n",
    "        for category in categories:\n",
    "            f.write(f\"{category['name']}\\n\")\n",
    "\n",
    "    print(f\"✅ Saved {len(categories)} classes to '{output_file}'\")\n",
    "\n",
    "# Example usage:\n",
    "extract_classes(r\"D:\\ALL\\Blance_augmented.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e1d1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
